Name,Description,Pros,Cons,Tips
DecisionTreeClassifier,"Decision Tree Classifier is a simple and widely used classification technique. It applies a straitforward idea to solve the classification problem. Decision Tree Classifier poses a series of carefully crafted questions about the attributes of the test record. Each time time it receive an answer, a follow-up question is asked until a conclusion about the calss label of the record is reached.","Compared to other algorithms decision trees requires less effort for data preparation during pre-processing. A decision tree does not require
normalization of data. A decision tree does not require scaling of data
as well.Missing values in the data also does NOT affect the process of building decision tree to any considerable extent.A Decision trees model
is very intuitive and easy to explain to technical teams as well as stakeholders.",A small change in the data can cause a large change in the structure of the decision tree causing instability. For a Decision tree sometimes calculation can go far more complex compared to other algorithms. Decision tree often involves higher time to train the model.Decision tree training is relatively expensive as complexity and time taken is more.Decision Tree algorithm is inadequate for applying regression and predicting continuous values.,Decision trees can become much more powerful when used as ensembles. Ensembles are clever ways of combining decision trees to create a more powerful model. These ensembles create state of the art machine learning algorithms that can outperform neural networks in some cases. The two most popular ensemble techniques are random forests and gradient boosting.
ExtraTreeClassifier,Extra Tree Classifier a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.,"builds multiple trees with bootstrap = False by default, which means it samples without replacement",nodes are split based on random splits among a random subset of the features selected at every node,"The Extra Trees classifier performs similar to the Random Forest. However, there are performance differences which is Decision Trees show high variance, Random Forests show medium variance, and Extra Trees show low variance."
BaggingClassifier,"A Bagging Classifier is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.",Bagging takes the advantage of ensemble learning wherein multiple weak learner outperform a single strong learner. It helps reduce variance and thus helps us avoid overfitting.,"There is loss of interpretability of the model. There can possibly be a problem of high bias if not modeled properly. Another important disadvantage is that while bagging gives us more accuracy, it is computationally expensive and may not be desirable depending on the use case.","Boosting and bagging are two ensemble methods capable of squeezing additional predictive accuracy out of classification algorithms. When using either method, careful tuning of the hyper-parameters should be done to find an optimal balance of model flexibility, efficiency & predictive improvement."
GradientBoostingClassifier,Gradient Boosting builds an additive model in a forward stage-wise fashion; It allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.,Often provides predictive accuracy that cannot be beat. Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible. No data pre-processing required - often works great with categorical and numerical values as is.,Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.,It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.
